{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow import keras\nimport os\nfrom keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,Flatten,Dropout,MaxPool2D,BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,EarlyStopping\n\nfrom glob import glob\nimport os\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-24T04:08:05.861848Z","iopub.execute_input":"2023-03-24T04:08:05.862289Z","iopub.status.idle":"2023-03-24T04:08:09.487967Z","shell.execute_reply.started":"2023-03-24T04:08:05.862249Z","shell.execute_reply":"2023-03-24T04:08:09.486868Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"emotion_to_test=[]\npath_to_test = []\nfor dirname, _, filenames in os.walk('/kaggle/input/brain-dead-emotion-detection/brain_dead_emotion_detection/Test'):\n    for filename in filenames:\n        path = os.path.join(dirname, filename)\n        emotion_to_test.append(path.split('/')[-2])\n        path_to_test.append(path)\n        \ntest = pd.DataFrame()\ntest['path']= path_to_test\ntest['emotion']= emotion_to_test","metadata":{"execution":{"iopub.status.busy":"2023-03-24T04:08:09.489443Z","iopub.execute_input":"2023-03-24T04:08:09.490268Z","iopub.status.idle":"2023-03-24T04:08:09.505327Z","shell.execute_reply.started":"2023-03-24T04:08:09.490226Z","shell.execute_reply":"2023-03-24T04:08:09.504153Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"emotion_to_train=[]\npath_to_train = []\nfor dirname, _, filenames in os.walk('/kaggle/input/brain-dead-emotion-detection/brain_dead_emotion_detection/Train'):\n    for filename in filenames:\n        path = os.path.join(dirname, filename)\n        emotion_to_train.append(path.split('/')[-2])\n        path_to_train.append(path)\ntrain = pd.DataFrame()\ntrain['path']= path_to_train\ntrain['emotion']= emotion_to_train","metadata":{"execution":{"iopub.status.busy":"2023-03-24T04:08:09.507483Z","iopub.execute_input":"2023-03-24T04:08:09.508583Z","iopub.status.idle":"2023-03-24T04:08:09.527205Z","shell.execute_reply.started":"2023-03-24T04:08:09.508518Z","shell.execute_reply":"2023-03-24T04:08:09.526052Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    num_classes = 4\n    width = 124\n    height = 124\n    channel = 3\n    batch_size=2\n    EPOCHS = 30\n    lr = 0.00001","metadata":{"execution":{"iopub.status.busy":"2023-03-24T04:12:17.208934Z","iopub.execute_input":"2023-03-24T04:12:17.209529Z","iopub.status.idle":"2023-03-24T04:12:17.214979Z","shell.execute_reply.started":"2023-03-24T04:12:17.209489Z","shell.execute_reply":"2023-03-24T04:12:17.213821Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"base_model = tf.keras.applications.xception.Xception(weights='imagenet', \n                                include_top=False, \n                                input_shape=(CFG.width,CFG.height ,CFG.channel ))\nbase_model.trainable = False\n\nx = base_model.output\nx = keras.layers.GlobalAveragePooling2D()(x)\n\nx = Dropout(0.5)(x)\n\nx = Dense(1024, activation='relu')(x)\nx = Dense(512, activation='relu')(x)\n\npredictions = Dense(CFG.num_classes, activation='softmax')(x)\n\nmodel = tf.keras.Model(base_model.input, predictions)\n\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-03-24T04:12:17.463620Z","iopub.execute_input":"2023-03-24T04:12:17.464633Z","iopub.status.idle":"2023-03-24T04:12:18.845424Z","shell.execute_reply.started":"2023-03-24T04:12:17.464582Z","shell.execute_reply":"2023-03-24T04:12:18.844326Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"TRAIN_DATAGEN = ImageDataGenerator(horizontal_flip=True,\n                                   rotation_range=25,\n                                   brightness_range=[0.15,.045],\n                                   rescale = 1./255,\n                )\n\nTEST_DATAGEN = ImageDataGenerator(\n                rescale = 1.0/255\n                )\n\ntrain_flow = TRAIN_DATAGEN.flow_from_dataframe(\n                                    dataframe=train,\n                                    x_col=\"path\",\n                                    y_col=\"emotion\",\n                                    subset=\"training\",\n                                    batch_size=CFG.batch_size,\n                                    seed=42,\n                                    shuffle=True,\n                                    class_mode=\"categorical\",\n                                    target_size=(CFG.width,CFG.height))\n\ntest_flow = TEST_DATAGEN.flow_from_dataframe(\n                                    dataframe=test,\n                                    x_col=\"path\",\n                                    y_col=\"emotion\",\n                                    subset=\"training\",\n                                    batch_size=CFG.batch_size,\n                                    seed=42,\n                                    shuffle=True,\n                                    class_mode=\"categorical\",\n                                    target_size=(CFG.width,CFG.height))\n\nTRAINING_NUM = train_flow.n \nVALID_NUM = test_flow.n\n\nSTEP_SIZE_TRAIN = TRAINING_NUM // CFG.batch_size \nSTEP_SIZE_VALID = VALID_NUM // CFG.batch_size\n\nmodel.compile(loss=tf.keras.losses.KLDivergence()\n              , optimizer=tf.keras.optimizers.Adam(CFG.lr), metrics=[tf.keras.metrics.Precision(name='precision')])\n\nreduce_lr=ReduceLROnPlateau(monitor='val_precision'\n                            ,factor=0.1,patience=5, min_delta=1e-3, verbose=1, min_lr=1e-6)\n\nweights=ModelCheckpoint('model_weights.hdf5',\n                       save_best_only=True,\n                       monitor='val_precision',\n                       verbose=1,\n                       save_weights_only=False\n                       )\n\nearly_stopping=EarlyStopping(monitor='val_precision',patience=5,\n                             restore_best_weights=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-24T04:12:18.850985Z","iopub.execute_input":"2023-03-24T04:12:18.853373Z","iopub.status.idle":"2023-03-24T04:12:18.998235Z","shell.execute_reply.started":"2023-03-24T04:12:18.853334Z","shell.execute_reply":"2023-03-24T04:12:18.997110Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Found 1810 validated image filenames belonging to 4 classes.\nFound 369 validated image filenames belonging to 4 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"\nhistory = model.fit(\n                    train_flow,\n                    validation_data=test_flow,\n                    epochs=CFG.EPOCHS,\n                    steps_per_epoch=STEP_SIZE_TRAIN,\n                    validation_steps=STEP_SIZE_VALID,\n                    callbacks=[reduce_lr,weights,early_stopping]\n                    )","metadata":{"execution":{"iopub.status.busy":"2023-03-24T04:12:19.002506Z","iopub.execute_input":"2023-03-24T04:12:19.002905Z","iopub.status.idle":"2023-03-24T04:14:11.026322Z","shell.execute_reply.started":"2023-03-24T04:12:19.002869Z","shell.execute_reply":"2023-03-24T04:14:11.025191Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Epoch 1/30\n904/905 [============================>.] - ETA: 0s - loss: 1.3735 - precision: 0.3000\nEpoch 1: val_precision improved from inf to 0.46429, saving model to model_weights.hdf5\n905/905 [==============================] - 22s 20ms/step - loss: 1.3735 - precision: 0.3000 - val_loss: 1.3455 - val_precision: 0.4643 - lr: 1.0000e-05\nEpoch 2/30\n904/905 [============================>.] - ETA: 0s - loss: 1.3238 - precision: 0.5789\nEpoch 2: val_precision did not improve from 0.46429\n905/905 [==============================] - 17s 19ms/step - loss: 1.3238 - precision: 0.5789 - val_loss: 1.2803 - val_precision: 0.5323 - lr: 1.0000e-05\nEpoch 3/30\n903/905 [============================>.] - ETA: 0s - loss: 1.2907 - precision: 0.5926\nEpoch 3: val_precision did not improve from 0.46429\n905/905 [==============================] - 16s 18ms/step - loss: 1.2908 - precision: 0.5926 - val_loss: 1.2487 - val_precision: 0.6119 - lr: 1.0000e-05\nEpoch 4/30\n902/905 [============================>.] - ETA: 0s - loss: 1.2572 - precision: 0.6324\nEpoch 4: val_precision did not improve from 0.46429\n905/905 [==============================] - 18s 20ms/step - loss: 1.2574 - precision: 0.6324 - val_loss: 1.2469 - val_precision: 0.5058 - lr: 1.0000e-05\nEpoch 5/30\n903/905 [============================>.] - ETA: 0s - loss: 1.2347 - precision: 0.6261\nEpoch 5: val_precision did not improve from 0.46429\n905/905 [==============================] - 17s 18ms/step - loss: 1.2345 - precision: 0.6277 - val_loss: 1.1912 - val_precision: 0.5698 - lr: 1.0000e-05\nEpoch 6/30\n904/905 [============================>.] - ETA: 0s - loss: 1.2013 - precision: 0.6973\nEpoch 6: ReduceLROnPlateau reducing learning rate to 1e-06.\n\nEpoch 6: val_precision did not improve from 0.46429\n905/905 [==============================] - 17s 19ms/step - loss: 1.2015 - precision: 0.6973 - val_loss: 1.1847 - val_precision: 0.5587 - lr: 1.0000e-05\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}